{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "3i1YCraj5bu_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (1) 형태소분석"
      ]
    },
    {
      "metadata": {
        "id": "sglkrFHe9WNG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>"
      ]
    },
    {
      "metadata": {
        "id": "bEP0QzsU9rE9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "< 파이썬 한국어 자연어처리 패키지인 KoNLPy 설치 >"
      ]
    },
    {
      "metadata": {
        "id": "-rImSuUwxOWf",
        "colab_type": "code",
        "outputId": "389dc3b1-31a9-4db9-bed9-e3cda2b07f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/3d/4e983cd98d87b50b2ab0387d73fa946f745aa8164e8888a714d5129f9765/konlpy-0.5.1-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 19.4MB 1.6MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.5.7 (from konlpy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/4b/60a3e63d51714d4d7ef1b1efdf84315d118a0a80a5b085bb52a7e2428cdc/JPype1-0.6.3.tar.gz (168kB)\n",
            "\u001b[K    100% |████████████████████████████████| 174kB 29.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: JPype1\n",
            "  Running setup.py bdist_wheel for JPype1 ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/0e/2b/e8/c0b818ac4b3d35104d35e48cdc7afe27fc06ea277feed2831a\n",
            "Successfully built JPype1\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-0.6.3 konlpy-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TOePqHjo-MQj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>"
      ]
    },
    {
      "metadata": {
        "id": "rGx4Weuk-IqC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "< 꼬꼬마로 형태소분석 ><br><br>\n",
        "nouns -> 명사만 추출<br>\n",
        "morphs -> 형태소 추출<br>\n",
        "pos -> 형태소와 품사 추출<br>"
      ]
    },
    {
      "metadata": {
        "id": "0vlnw2wIoijw",
        "colab_type": "code",
        "outputId": "3fc4b8f9-59d2-4ab2-e5b5-395325c3f4ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "print(kkma.nouns(u'자연어처리는 컴퓨터가 인간의 언어를 처리하도록 하는 인공지능입니다.'))\n",
        "print(kkma.morphs(u'자연어처리는 컴퓨터가 인간의 언어를 처리하도록 하는 인공지능입니다.'))\n",
        "print(kkma.pos(u'자연어처리는 컴퓨터가 인간의 언어를 처리하도록 하는 인공지능입니다.'))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['자연어', '자연어처리', '처리', '컴퓨터', '인간', '언어', '인공', '인공지능', '지능']\n",
            "['자연어', '처리', '는', '컴퓨터', '가', '인간', '의', '언어', '를', '처리', '하', '도록', '하', '는', '인공지능', '이', 'ㅂ니다', '.']\n",
            "[('자연어', 'NNG'), ('처리', 'NNG'), ('는', 'JX'), ('컴퓨터', 'NNG'), ('가', 'JKS'), ('인간', 'NNG'), ('의', 'JKG'), ('언어', 'NNG'), ('를', 'JKO'), ('처리', 'NNG'), ('하', 'XSV'), ('도록', 'ECD'), ('하', 'VV'), ('는', 'ETD'), ('인공지능', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1UjpVR33-wOz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>"
      ]
    },
    {
      "metadata": {
        "id": "KfkMzFuj-qPa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "< 코모란으로 형태소분석 >"
      ]
    },
    {
      "metadata": {
        "id": "_qESEz8Gy55G",
        "colab_type": "code",
        "outputId": "5576e47f-7805-4e36-df3b-084158e3a9ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "komoran = Komoran()\n",
        "\n",
        "print(komoran.nouns(u'자연어처리는 컴퓨터가 인간의 언어를 처리하도록 하는 인공지능입니다.'))\n",
        "print(komoran.morphs(u'자연어처리는 컴퓨터가 인간의 언어를 처리하도록 하는 인공지능입니다.'))\n",
        "print(komoran.pos(u'자연어처리는 컴퓨터가 인간의 언어를 처리하도록 하는 인공지능입니다.'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['자연어', '처리', '컴퓨터', '인간', '언어', '처리', '인공지능']\n",
            "['자연어', '처리', '는', '컴퓨터', '가', '인간', '의', '언어', '를', '처리', '하', '도록', '하', '는', '인공지능', '이', 'ㅂ니다', '.']\n",
            "[('자연어', 'NNP'), ('처리', 'NNP'), ('는', 'JX'), ('컴퓨터', 'NNG'), ('가', 'JKS'), ('인간', 'NNG'), ('의', 'JKG'), ('언어', 'NNG'), ('를', 'JKO'), ('처리', 'NNG'), ('하', 'XSV'), ('도록', 'EC'), ('하', 'VV'), ('는', 'ETM'), ('인공지능', 'NNP'), ('이', 'VCP'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OcE3l8jh8cx_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>\n",
        "  <br>"
      ]
    },
    {
      "metadata": {
        "id": "WnNfjph87Azs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (2) RNN으로 캐릭터 글자 예측"
      ]
    },
    {
      "metadata": {
        "id": "ze2j59egDCIC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>"
      ]
    },
    {
      "metadata": {
        "id": "TUuogKYIDE4t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "< 원본 코드 ><br>\n",
        "-> https://github.com/golbin/TensorFlow-Tutorials/blob/master/10%20-%20RNN/02%20-%20Autocomplete.py<br>\n",
        "<br><br><br>\n",
        "< 프로그램 설명 ><br>\n",
        "<br>\n",
        "4개의 글자를 가진 단어를 학습<br>\n",
        "3글자가 주어지면 마지막 글자를 예측<br>\n",
        "<br>\n",
        "-----------------<br>\n",
        "wor -> d<br>\n",
        "lov -> e<br>\n",
        "-----------------<br>\n",
        "<br>\n",
        "(에러 발생시 '런타임->모든 런타임 재설정' 후 처음부터 실행)\n",
        "<br><br><br>"
      ]
    },
    {
      "metadata": {
        "id": "-Sc1QelvSRUX",
        "colab_type": "code",
        "outputId": "22410fc7-7da6-4364-af0e-be1b9c00dad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# 캐릭터 설정\n",
        "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
        "            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
        "            'o', 'p', 'q', 'r', 's', 't', 'u',\n",
        "            'v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "# One-hot 인코딩 사용 및 디코딩을 하기 위해 연관 배열을 만듬\n",
        "# {'a': 0, 'b': 1, 'c': 2, ..., 'j': 9, 'k', 10, ...}\n",
        "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
        "dic_len = len(num_dic)\n",
        "\n",
        "# 4자로 된 단어 집합\n",
        "# wor -> X, d -> Y\n",
        "# woo -> X, d -> Y\n",
        "seq_data = ['word', 'wood', 'deep', 'dive', 'cold', \n",
        "            'cool', 'load', 'love', 'kiss', 'kind']\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------\n",
        "# 데이터 배치 생성\n",
        "#----------------------------------------\n",
        "def make_batch(seq_data):\n",
        "\n",
        "    input_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for seq in seq_data:\n",
        "        # input_batch와 target_batch는 알파벳 배열의 인덱스 번호\n",
        "        # [22, 14, 17] [22, 14, 14] [3, 4, 4] [3, 8, 21] ...\n",
        "        input = [num_dic[n] for n in seq[:-1]]\n",
        "        \n",
        "        # 3, 3, 15, 4, 3 ...\n",
        "        target = num_dic[seq[-1]]\n",
        "        \n",
        "        # One-hot 인코딩\n",
        "        # if input is [0, 1, 2]:\n",
        "        # [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        "        #  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        "        #  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n",
        "        input_batch.append(np.eye(dic_len)[input])\n",
        "        \n",
        "        # 비용함수의 label 값\n",
        "        # softmax_cross_entropy_with_logits -> one-hot 인코딩\n",
        "        # sparse_softmax_cross_entropy_with_logits -> index 숫자\n",
        "        target_batch.append(target)\n",
        "\n",
        "    return input_batch, target_batch\n",
        "\n",
        "\n",
        "  \n",
        "#----------------------------------------\n",
        "# 첫번째 배치 출력\n",
        "# input : w, o, r\n",
        "# target : d\n",
        "#----------------------------------------\n",
        "input_batch, target_batch = make_batch(seq_data)\n",
        "\n",
        "print(input_batch[0])\n",
        "print('\\n')\n",
        "print(target_batch[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0.]]\n",
            "\n",
            "\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LKYRK-cOMHi5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ADuzj2qcnWyY",
        "colab_type": "code",
        "outputId": "d743daa4-7b8a-4425-b563-4118ffd3cdd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "cell_type": "code",
      "source": [
        "#----------------------------------------\n",
        "# 옵션 설정\n",
        "#----------------------------------------\n",
        "learning_rate = 0.01\n",
        "n_hidden = 128\n",
        "total_epoch = 30\n",
        "\n",
        "# 타입 스텝: [1 2 3] => 3\n",
        "# RNN을 구성하는 시퀀스의 갯수\n",
        "n_step = 3\n",
        "\n",
        "# 입력값과 출력값의 크기\n",
        "# 알파벳에 대한 one-hot 인코딩이므로 26개\n",
        "# 예) c => [0 0 1 0 0 0 0 0 0 0 0 ... 0]\n",
        "n_input = n_class = dic_len\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------\n",
        "# 신경망 모델 구성\n",
        "#----------------------------------------\n",
        "X = tf.placeholder(tf.float32, [None, n_step, n_input])\n",
        "Y = tf.placeholder(tf.int32, [None])\n",
        "W = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
        "b = tf.Variable(tf.random_normal([n_class]))\n",
        "\n",
        "# RNN 셀\n",
        "cell1 = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
        "\n",
        "# 과적합 방지를 위한 Dropout\n",
        "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
        "\n",
        "# 여러개의 셀을 조합해서 사용하기 위해 셀을 추가로 생성\n",
        "cell2 = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
        "\n",
        "# 여러개의 셀을 조합한 RNN 셀을 생성\n",
        "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
        "\n",
        "# 순환 신경망을 생성\n",
        "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\n",
        "\n",
        "# outputs 결과에서 마지막 타임 스텝만 구함\n",
        "# outputs의 형태를 변경\n",
        "# outputs : [batch_size, n_step, n_hidden]\n",
        "#        -> [n_step, batch_size, n_hidden]\n",
        "#        -> [batch_size, n_hidden]\n",
        "outputs = tf.transpose(outputs, [1, 0, 2])\n",
        "outputs = outputs[-1]\n",
        "\n",
        "# 128개 히든 레이어의 output를 26개 알파벳의 one-hot 인코딩 형식으로 변경\n",
        "model = tf.matmul(outputs, W) + b\n",
        "\n",
        "# 비용함수\n",
        "cost = tf.reduce_mean(\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                logits=model, labels=Y))\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------\n",
        "# 신경망 모델 학습\n",
        "#----------------------------------------\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "input_batch, target_batch = make_batch(seq_data)\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    _, loss = sess.run([optimizer, cost],\n",
        "                       feed_dict={X: input_batch, Y: target_batch})\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1),\n",
        "          'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "print('최적화 완료!')\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------\n",
        "# 테스트\n",
        "#----------------------------------------\n",
        "\n",
        "# 레이블값이 정수이므로 예측값도 정수로 변경\n",
        "prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
        "\n",
        "# one-hot 인코딩이 아니므로 입력값을 그대로 비교\n",
        "prediction_check = tf.equal(prediction, Y)\n",
        "accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32))\n",
        "\n",
        "input_batch, target_batch = make_batch(seq_data)\n",
        "\n",
        "predict, accuracy_val = sess.run([prediction, accuracy],\n",
        "                                 feed_dict={X: input_batch, \n",
        "                                            Y: target_batch})\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------\n",
        "# 테스트 결과 출력\n",
        "#----------------------------------------\n",
        "predict_words = []\n",
        "\n",
        "for idx, val in enumerate(seq_data):\n",
        "    last_char = char_arr[predict[idx]]\n",
        "    predict_words.append(val[:3] + last_char)\n",
        "    \n",
        "print('\\n=== 예측 결과 ===')\n",
        "print('입력값:', [w[:3] + ' ' for w in seq_data])\n",
        "print('예측값:', predict_words)\n",
        "print('정확도:', accuracy_val)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 3.428905\n",
            "Epoch: 0002 cost = 2.370727\n",
            "Epoch: 0003 cost = 1.275206\n",
            "Epoch: 0004 cost = 1.216647\n",
            "Epoch: 0005 cost = 1.153481\n",
            "Epoch: 0006 cost = 0.393047\n",
            "Epoch: 0007 cost = 0.566710\n",
            "Epoch: 0008 cost = 0.652971\n",
            "Epoch: 0009 cost = 0.428852\n",
            "Epoch: 0010 cost = 0.347449\n",
            "Epoch: 0011 cost = 0.216702\n",
            "Epoch: 0012 cost = 0.314815\n",
            "Epoch: 0013 cost = 0.133950\n",
            "Epoch: 0014 cost = 0.108295\n",
            "Epoch: 0015 cost = 0.203173\n",
            "Epoch: 0016 cost = 0.133532\n",
            "Epoch: 0017 cost = 0.126507\n",
            "Epoch: 0018 cost = 0.046505\n",
            "Epoch: 0019 cost = 0.221982\n",
            "Epoch: 0020 cost = 0.056451\n",
            "Epoch: 0021 cost = 0.017648\n",
            "Epoch: 0022 cost = 0.144496\n",
            "Epoch: 0023 cost = 0.098071\n",
            "Epoch: 0024 cost = 0.115524\n",
            "Epoch: 0025 cost = 0.043173\n",
            "Epoch: 0026 cost = 0.075338\n",
            "Epoch: 0027 cost = 0.028429\n",
            "Epoch: 0028 cost = 0.007997\n",
            "Epoch: 0029 cost = 0.044977\n",
            "Epoch: 0030 cost = 0.020536\n",
            "최적화 완료!\n",
            "\n",
            "=== 예측 결과 ===\n",
            "입력값: ['wor ', 'woo ', 'dee ', 'div ', 'col ', 'coo ', 'loa ', 'lov ', 'kis ', 'kin ']\n",
            "예측값: ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
            "정확도: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0WXIAzDWvmjy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "< transpose 추가 설명 >"
      ]
    },
    {
      "metadata": {
        "id": "YE2274uLlGyx",
        "colab_type": "code",
        "outputId": "6b6707e6-dc81-4b10-91c9-1259edb637b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "# outputs 결과에서 마지막 타임 스텝만 구함\n",
        "# outputs의 형태를 변경\n",
        "# outputs : [batch_size, n_step, n_hidden]\n",
        "#        -> [n_step, batch_size, n_hidden]\n",
        "#        -> [batch_size, n_hidden]\n",
        "#\n",
        "# batch_size -> 2\n",
        "# n_step -> 3\n",
        "# n_hidden -> 2\n",
        "outputs = [[[1,2], [3,4], [5,6]], [[11,12], [13,14], [15,16]]]\n",
        "\n",
        "# 행렬의 차원을 [0, 1, 2]에서 [1, 0, 2]로 변경\n",
        "outputs = np.transpose(outputs, [1, 0, 2])\n",
        "print(outputs)\n",
        "\n",
        "# n_step을 삭제하고 마지막 타임 스텝만 구함\n",
        "outputs = outputs[-1]\n",
        "print('\\n')\n",
        "print(outputs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 1  2]\n",
            "  [11 12]]\n",
            "\n",
            " [[ 3  4]\n",
            "  [13 14]]\n",
            "\n",
            " [[ 5  6]\n",
            "  [15 16]]]\n",
            "\n",
            "\n",
            "[[ 5  6]\n",
            " [15 16]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z6Um57IRy6b-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "metadata": {
        "id": "qJElgNSLzBDv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (3) Seq2Seq로 단어 번역"
      ]
    },
    {
      "metadata": {
        "id": "a6dMTSotI4W6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>"
      ]
    },
    {
      "metadata": {
        "id": "WX1adnUYIza4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "< 원본 코드 ><br>\n",
        "-> https://github.com/golbin/TensorFlow-Tutorials/blob/master/10%20-%20RNN/03%20-%20Seq2Seq.py<br>\n",
        "<br><br><br>\n",
        "< 프로그램 설명 ><br>\n",
        "<br>\n",
        "영어 단어를 한글로 번역<br>\n",
        "<br>\n",
        "--------------------------<br>\n",
        "wood -> 나무<br>\n",
        "game -> 놀이<br>\n",
        "--------------------------<br>\n",
        "<br><br><br>"
      ]
    },
    {
      "metadata": {
        "id": "_X_qaFZCVAV1",
        "colab_type": "code",
        "outputId": "9335bfc2-9097-4c4a-e631-421f1986f354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# S: 디코딩 입력의 시작을 나타내는 심볼\n",
        "# E: 디코딩 출력을 끝을 나타내는 심볼\n",
        "# P: 현재 배치 데이터의 time step 크기보다 작은 경우 빈 시퀀스를 채우는 심볼\n",
        "#    예) 현재 배치 데이터의 최대 크기가 4 인 경우\n",
        "#       word -> ['w', 'o', 'r', 'd']\n",
        "#       to   -> ['t', 'o', 'P', 'P']\n",
        "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']\n",
        "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
        "dic_len = len(num_dic)\n",
        "\n",
        "# 영어를 한글로 번역하기 위한 학습 데이터\n",
        "seq_data = [['word', '단어'], ['wood', '나무'],\n",
        "            ['game', '놀이'], ['girl', '소녀'],\n",
        "            ['kiss', '키스'], ['love', '사랑']]\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------\n",
        "# 데이터 배치 생성\n",
        "#----------------------------------------\n",
        "def make_batch(seq_data):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for seq in seq_data:\n",
        "        # 인코더 셀의 입력값. 입력단어의 글자들을 한글자씩 떼어 배열로 만듬.\n",
        "        input = [num_dic[n] for n in seq[0]]\n",
        "        \n",
        "        # 디코더 셀의 입력값. 시작을 나타내는 S 심볼을 맨 앞에 붙여줌.\n",
        "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
        "        \n",
        "        # 학습을 위해 비교할 디코더 셀의 출력값. 끝나는 것을 알려주기 위해 마지막에 E 를 붙임.\n",
        "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
        "\n",
        "        # One-hot 인코딩으로 배치 추가\n",
        "        input_batch.append(np.eye(dic_len)[input])\n",
        "        output_batch.append(np.eye(dic_len)[output])\n",
        "        \n",
        "        # 출력값만 one-hot 인코딩이 아님(sparse_softmax_cross_entropy_with_logits 사용)\n",
        "        target_batch.append(target)\n",
        "\n",
        "    return input_batch, output_batch, target_batch\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------\n",
        "# 첫번째 배치 출력\n",
        "# input : w, o, r, d\n",
        "# output : S, 단, 어\n",
        "# target : 단, 어, E\n",
        "#----------------------------------------\n",
        "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
        "\n",
        "print(input_batch[0])\n",
        "print('\\n')\n",
        "print(output_batch[0])\n",
        "print('\\n')\n",
        "print(target_batch[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "[29, 30, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m3VeOPyRWP8E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>"
      ]
    },
    {
      "metadata": {
        "id": "1iO6j2WWJndz",
        "colab_type": "code",
        "outputId": "6852eb27-35cf-4f37-fbce-9cc326f90da5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2033
        }
      },
      "cell_type": "code",
      "source": [
        "#----------------------------------------\n",
        "# 옵션 설정\n",
        "#----------------------------------------\n",
        "learning_rate = 0.01\n",
        "n_hidden = 128\n",
        "total_epoch = 100\n",
        "\n",
        "# 입력과 출력의 형태가 one-hot 인코딩으로 같으므로 크기도 같음\n",
        "n_class = n_input = dic_len\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------\n",
        "# 신경망 모델 구성\n",
        "#----------------------------------------\n",
        "\n",
        "# Seq2Seq 모델은 인코더의 입력과 디코더의 입력의 형식이 같음\n",
        "# [batch size, time steps, input size]\n",
        "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "\n",
        "# [batch size, time steps]\n",
        "targets = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "# 인코더 셀을 구성한다.\n",
        "with tf.variable_scope('encode'):\n",
        "    enc_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
        "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, \n",
        "                                             output_keep_prob=0.5)\n",
        "\n",
        "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
        "                                            dtype=tf.float32)\n",
        "\n",
        "# 디코더 셀을 구성한다.\n",
        "with tf.variable_scope('decode'):\n",
        "    dec_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
        "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, \n",
        "                                             output_keep_prob=0.5)\n",
        "\n",
        "    # Seq2Seq 모델은 인코더 셀의 최종 상태값을 디코더 셀의 초기 상태값으로 넣어주는 것이 핵심\n",
        "    # 인코더 셀과 달리 initial_state에 enc_states를 설정\n",
        "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
        "                                            initial_state=enc_states,\n",
        "                                            dtype=tf.float32)\n",
        "\n",
        "# 이전 RNN으로 캐릭터 글자 예측과 달리 타입 스텝을 삭제하지 않음\n",
        "# 각 타임 스텝마다 단어 출력\n",
        "model = tf.layers.dense(outputs, n_class, activation=None)\n",
        "\n",
        "cost = tf.reduce_mean(\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                logits=model, labels=targets))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------\n",
        "# 신경망 모델 학습\n",
        "#----------------------------------------\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    _, loss = sess.run([optimizer, cost],\n",
        "                       feed_dict={enc_input: input_batch,\n",
        "                                  dec_input: output_batch,\n",
        "                                  targets: target_batch})\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1),\n",
        "          'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "print('최적화 완료!')\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------\n",
        "# 단어를 입력받아 번역 단어를 예측하고 디코딩\n",
        "#----------------------------------------\n",
        "def translate(word):\n",
        "\n",
        "    # 이 모델은 입력값과 출력값 데이터로 [영어단어, 한글단어] 사용하지만,\n",
        "    # 예측시에는 한글단어를 알지 못하므로 디코더의 입출력값을 의미 없는 값인 P 값으로 채움\n",
        "    # ['word', 'PPPP']\n",
        "    seq_data = [word, 'P' * len(word)]\n",
        "\n",
        "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
        "\n",
        "    # 결과가 [batch size, time step, input] 으로 나오기 때문에,\n",
        "    # 2번째 차원인 input 차원을 argmax로 취해 가장 확률이 높은 글자를 예측 값으로 만듬\n",
        "    prediction = tf.argmax(model, 2)\n",
        "\n",
        "    result = sess.run(prediction,\n",
        "                      feed_dict={enc_input: input_batch,\n",
        "                                 dec_input: output_batch,\n",
        "                                 targets: target_batch})\n",
        "\n",
        "    # 결과 값인 숫자의 인덱스에 해당하는 글자를 가져와 글자 배열을 만든다.\n",
        "    decoded = [char_arr[i] for i in result[0]]\n",
        "    \n",
        "    # 출력의 끝을 의미하는 'E' 이후의 글자들을 제거하고 문자열로 만든다.\n",
        "    end = decoded.index('E')\n",
        "    translated = ''.join(decoded[:end])\n",
        "\n",
        "    return translated\n",
        "\n",
        "\n",
        "\n",
        "print('\\n=== 번역 테스트 ===')\n",
        "\n",
        "print('word ->', translate('word'))\n",
        "print('wodr ->', translate('wodr'))\n",
        "print('love ->', translate('love'))\n",
        "print('loev ->', translate('loev'))\n",
        "print('abcd ->', translate('abcd'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 3.714662\n",
            "Epoch: 0002 cost = 3.590534\n",
            "Epoch: 0003 cost = 3.330037\n",
            "Epoch: 0004 cost = 2.878475\n",
            "Epoch: 0005 cost = 2.439181\n",
            "Epoch: 0006 cost = 2.378341\n",
            "Epoch: 0007 cost = 2.059747\n",
            "Epoch: 0008 cost = 1.868184\n",
            "Epoch: 0009 cost = 1.763801\n",
            "Epoch: 0010 cost = 1.572342\n",
            "Epoch: 0011 cost = 1.253408\n",
            "Epoch: 0012 cost = 1.371611\n",
            "Epoch: 0013 cost = 1.031342\n",
            "Epoch: 0014 cost = 0.919781\n",
            "Epoch: 0015 cost = 0.831009\n",
            "Epoch: 0016 cost = 0.784351\n",
            "Epoch: 0017 cost = 0.724733\n",
            "Epoch: 0018 cost = 0.681761\n",
            "Epoch: 0019 cost = 0.658159\n",
            "Epoch: 0020 cost = 0.427962\n",
            "Epoch: 0021 cost = 0.423981\n",
            "Epoch: 0022 cost = 0.451285\n",
            "Epoch: 0023 cost = 0.379398\n",
            "Epoch: 0024 cost = 0.477406\n",
            "Epoch: 0025 cost = 0.305556\n",
            "Epoch: 0026 cost = 0.330127\n",
            "Epoch: 0027 cost = 0.288085\n",
            "Epoch: 0028 cost = 0.254057\n",
            "Epoch: 0029 cost = 0.285114\n",
            "Epoch: 0030 cost = 0.292090\n",
            "Epoch: 0031 cost = 0.184193\n",
            "Epoch: 0032 cost = 0.323169\n",
            "Epoch: 0033 cost = 0.194645\n",
            "Epoch: 0034 cost = 0.270495\n",
            "Epoch: 0035 cost = 0.157759\n",
            "Epoch: 0036 cost = 0.170117\n",
            "Epoch: 0037 cost = 0.137572\n",
            "Epoch: 0038 cost = 0.124903\n",
            "Epoch: 0039 cost = 0.135195\n",
            "Epoch: 0040 cost = 0.139586\n",
            "Epoch: 0041 cost = 0.089807\n",
            "Epoch: 0042 cost = 0.095406\n",
            "Epoch: 0043 cost = 0.063997\n",
            "Epoch: 0044 cost = 0.034280\n",
            "Epoch: 0045 cost = 0.072192\n",
            "Epoch: 0046 cost = 0.099852\n",
            "Epoch: 0047 cost = 0.036045\n",
            "Epoch: 0048 cost = 0.062551\n",
            "Epoch: 0049 cost = 0.055762\n",
            "Epoch: 0050 cost = 0.022394\n",
            "Epoch: 0051 cost = 0.044899\n",
            "Epoch: 0052 cost = 0.039242\n",
            "Epoch: 0053 cost = 0.039374\n",
            "Epoch: 0054 cost = 0.021592\n",
            "Epoch: 0055 cost = 0.047190\n",
            "Epoch: 0056 cost = 0.015489\n",
            "Epoch: 0057 cost = 0.018375\n",
            "Epoch: 0058 cost = 0.024381\n",
            "Epoch: 0059 cost = 0.046030\n",
            "Epoch: 0060 cost = 0.012608\n",
            "Epoch: 0061 cost = 0.016129\n",
            "Epoch: 0062 cost = 0.013914\n",
            "Epoch: 0063 cost = 0.019508\n",
            "Epoch: 0064 cost = 0.021680\n",
            "Epoch: 0065 cost = 0.017655\n",
            "Epoch: 0066 cost = 0.010324\n",
            "Epoch: 0067 cost = 0.006433\n",
            "Epoch: 0068 cost = 0.017130\n",
            "Epoch: 0069 cost = 0.007960\n",
            "Epoch: 0070 cost = 0.005492\n",
            "Epoch: 0071 cost = 0.006890\n",
            "Epoch: 0072 cost = 0.006543\n",
            "Epoch: 0073 cost = 0.012334\n",
            "Epoch: 0074 cost = 0.010032\n",
            "Epoch: 0075 cost = 0.008273\n",
            "Epoch: 0076 cost = 0.007317\n",
            "Epoch: 0077 cost = 0.004774\n",
            "Epoch: 0078 cost = 0.008529\n",
            "Epoch: 0079 cost = 0.006724\n",
            "Epoch: 0080 cost = 0.002828\n",
            "Epoch: 0081 cost = 0.008370\n",
            "Epoch: 0082 cost = 0.013691\n",
            "Epoch: 0083 cost = 0.002287\n",
            "Epoch: 0084 cost = 0.005495\n",
            "Epoch: 0085 cost = 0.005408\n",
            "Epoch: 0086 cost = 0.005895\n",
            "Epoch: 0087 cost = 0.003524\n",
            "Epoch: 0088 cost = 0.002967\n",
            "Epoch: 0089 cost = 0.002511\n",
            "Epoch: 0090 cost = 0.007450\n",
            "Epoch: 0091 cost = 0.003764\n",
            "Epoch: 0092 cost = 0.005128\n",
            "Epoch: 0093 cost = 0.003055\n",
            "Epoch: 0094 cost = 0.006667\n",
            "Epoch: 0095 cost = 0.003258\n",
            "Epoch: 0096 cost = 0.004313\n",
            "Epoch: 0097 cost = 0.001461\n",
            "Epoch: 0098 cost = 0.001227\n",
            "Epoch: 0099 cost = 0.002539\n",
            "Epoch: 0100 cost = 0.003326\n",
            "최적화 완료!\n",
            "\n",
            "=== 번역 테스트 ===\n",
            "word -> 단어\n",
            "wodr -> 단어\n",
            "love -> 사랑\n",
            "loev -> 사랑\n",
            "abcd -> 놀이\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}